{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from utils import normalize_rgb, render_meshes, get_focalLength_from_fieldOfView, demo_color as color, print_distance_on_image, render_side_views, create_scene, MEAN_PARAMS, CACHE_DIR_MULTIHMR, SMPLX_DIR\n",
    "from model import Model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions from demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(img_path, img_size, device=torch.device('cuda')):\n",
    "    \"\"\" Open image at path, resize and pad \"\"\"\n",
    "\n",
    "    # Open and reshape\n",
    "    img_pil = Image.open(img_path).convert('RGB')\n",
    "    img_pil = ImageOps.contain(img_pil, (img_size,img_size)) # keep the same aspect ratio\n",
    "\n",
    "    # Keep a copy for visualisations.\n",
    "    img_pil_bis = ImageOps.pad(img_pil.copy(), size=(img_size,img_size), color=(255, 255, 255))\n",
    "    img_pil = ImageOps.pad(img_pil, size=(img_size,img_size)) # pad with zero on the smallest side\n",
    "\n",
    "    # Go to numpy \n",
    "    resize_img = np.asarray(img_pil)\n",
    "\n",
    "    # Normalize and go to torch. MODIFIED TO NOT GOT TO TORCH\n",
    "    resize_img = normalize_rgb(resize_img)\n",
    "    x = np.expand_dims(resize_img, axis=0)\n",
    "    return x, img_pil_bis\n",
    "\n",
    "def load_model(model_name, device=torch.device('cuda')):\n",
    "    \"\"\" Open a checkpoint, build Multi-HMR using saved arguments, load the model weigths. \"\"\"\n",
    "    # Model\n",
    "    ckpt_path = os.path.join(CACHE_DIR_MULTIHMR, model_name+ '.pt')\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        os.makedirs(CACHE_DIR_MULTIHMR, exist_ok=True)\n",
    "        print(f\"{ckpt_path} not found...\")\n",
    "        print(\"It should be the first time you run the demo code\")\n",
    "        print(\"Downloading checkpoint from NAVER LABS Europe website...\")\n",
    "        \n",
    "        try:\n",
    "            os.system(f\"wget -O {ckpt_path} https://download.europe.naverlabs.com/ComputerVision/MultiHMR/{model_name}.pt\")\n",
    "            print(f\"Ckpt downloaded to {ckpt_path}\")\n",
    "        except:\n",
    "            assert \"Please contact fabien.baradel@naverlabs.com or open an issue on the github repo\"\n",
    "\n",
    "    # Load weights\n",
    "    print(\"Loading model\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # Get arguments saved in the checkpoint to rebuild the model\n",
    "    kwargs = {}\n",
    "    for k,v in vars(ckpt['args']).items():\n",
    "            kwargs[k] = v\n",
    "\n",
    "    # Build the model.\n",
    "    kwargs['type'] = ckpt['args'].train_return_type\n",
    "    kwargs['img_size'] = ckpt['args'].img_size[0]\n",
    "    model = Model(**kwargs).to(device)\n",
    "\n",
    "    # Load weights into model.\n",
    "    model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    print(\"Weights have been loaded\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def forward_model(model, input_image, camera_parameters,\n",
    "                  det_thresh=0.3,\n",
    "                  nms_kernel_size=1,\n",
    "                 ):\n",
    "        \n",
    "    \"\"\" Make a forward pass on an input image and camera parameters. \"\"\"\n",
    "    \n",
    "    # Forward the model.\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            humans = model(input_image, \n",
    "                           is_training=False, \n",
    "                           nms_kernel_size=int(nms_kernel_size),\n",
    "                           det_thresh=det_thresh,\n",
    "                           K=camera_parameters)\n",
    "\n",
    "    return humans\n",
    "\n",
    "def get_camera_parameters(img_size, fov=60, p_x=None, p_y=None, device=torch.device('cuda')):\n",
    "    \"\"\" Given image size, fov and principal point coordinates, return K the camera parameter matrix\"\"\"\n",
    "    K = torch.eye(3)\n",
    "    # Get focal length.\n",
    "    focal = get_focalLength_from_fieldOfView(fov=fov, img_size=img_size)\n",
    "    K[0,0], K[1,1] = focal, focal\n",
    "\n",
    "    # Set principal point\n",
    "    if p_x is not None and p_y is not None:\n",
    "            K[0,-1], K[1,-1] = p_x * img_size, p_y * img_size\n",
    "    else:\n",
    "            K[0,-1], K[1,-1] = img_size//2, img_size//2\n",
    "\n",
    "    # Add batch dimension\n",
    "    K = K.unsqueeze(0).to(device)\n",
    "    return K\n",
    "\n",
    "def overlay_human_meshes(humans, K, model, img_pil, unique_color=False):\n",
    "\n",
    "    # Color of humans seen in the image.\n",
    "    _color = [color[0] for _ in range(len(humans))] if unique_color else color\n",
    "    \n",
    "    # Get focal and princpt for rendering.\n",
    "    focal = np.asarray([K[0,0,0].cpu().numpy(),K[0,1,1].cpu().numpy()])\n",
    "    princpt = np.asarray([K[0,0,-1].cpu().numpy(),K[0,1,-1].cpu().numpy()])\n",
    "\n",
    "    # Get the vertices produced by the model.\n",
    "    verts_list = [humans[j]['verts_smplx'].cpu().numpy() for j in range(len(humans))]\n",
    "    faces_list = [model.smpl_layer['neutral'].bm_x.faces for j in range(len(humans))]\n",
    "\n",
    "    # Render the meshes onto the image.\n",
    "    pred_rend_array = render_meshes(np.asarray(img_pil), \n",
    "            verts_list,\n",
    "            faces_list,\n",
    "            {'focal': focal, 'princpt': princpt},\n",
    "            alpha=1.0,\n",
    "            color=_color)\n",
    "\n",
    "    return pred_rend_array, _color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/scott/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have been loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_model('multiHMR_896_L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AGORA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = model.img_size\n",
    "\n",
    "train_x_path = \"AGORA/train_0\"\n",
    "train_x = [] # images\n",
    "train_y = [] # ground truth mesh vertices\n",
    "\n",
    "with open(\"AGORA/SMPLX/train_0_withjv.pkl\", \"rb\") as file:\n",
    "    df = pd.read_pickle(file)\n",
    "    for filename in os.listdir(train_x_path):\n",
    "        file_path = os.path.join(train_x_path, filename)\n",
    "        x, img_pil_nopad = open_image(file_path, img_size)\n",
    "        train_x.append(x)\n",
    "        y = df[df['imgPath'] == filename.replace(\"_1280x720\", \"\")]\n",
    "        train_y.append(np.array(y['gt_verts'][0]))\n",
    "\n",
    "assert len(train_x) == len(train_y) == 1453 # Size of AGORA/train0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra humans based on detection score\n",
    "from copy import copy\n",
    "def align_humans(predictions, gts):\n",
    "    predictions = copy(predictions)\n",
    "    gts = copy(gts)\n",
    "\n",
    "    aligned_preds = []\n",
    "    aligned_gts = []\n",
    "    for pred, gt in zip(predictions, gts):\n",
    "        while len(pred) > len(gt):\n",
    "            det_scores = [person['scores'] for person in pred]\n",
    "            min_value = min(det_scores, key=lambda x: x.item())\n",
    "            index = det_scores.index(min_value)\n",
    "            pred = pred[:index] + pred[index+1:]\n",
    "        while len(pred) < len(gt):\n",
    "            gt = gt[:len(gt)-1]\n",
    "            \n",
    "        assert len(pred) == len(gt)\n",
    "        aligned_preds.append(pred)\n",
    "        aligned_gts.append(gt)\n",
    "\n",
    "    assert len(aligned_preds) == len(aligned_gts)\n",
    "    return aligned_preds, aligned_gts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertices Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, gts):\n",
    "    # generate array of only vertex information\n",
    "    pred_humans_with_only_vertices = []\n",
    "\n",
    "    count = 0\n",
    "    for humans in predictions:\n",
    "        pred_vertices = []\n",
    "        for human in humans:\n",
    "            pred_vertices.append(human['verts_smplx'])\n",
    "            count += 1\n",
    "        pred_humans_with_only_vertices.append(pred_vertices)\n",
    "    \n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    # convert to tensors\n",
    "    pred_humans_with_only_vertices = torch.stack([tensor for sublist in pred_humans_with_only_vertices for tensor in sublist])\n",
    "    gts = torch.from_numpy(np.stack([item for sublist in gts for item in sublist]))\n",
    "\n",
    "    pred_humans_with_only_vertices = pred_humans_with_only_vertices.to(device)\n",
    "    gts = gts.to(device)\n",
    "\n",
    "    print(f\"number of humans detected: {count}\")\n",
    "\n",
    "    return criterion(pred_humans_with_only_vertices, gts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of humans detected: 711\n",
      "tensor([[[ 2.1931, -3.2363, 20.5538],\n",
      "         [ 2.1914, -3.2359, 20.5578],\n",
      "         [ 2.1911, -3.2359, 20.5588],\n",
      "         ...,\n",
      "         [ 2.2859, -3.1965, 20.5016],\n",
      "         [ 2.2848, -3.1983, 20.5030],\n",
      "         [ 2.2836, -3.1995, 20.5047]],\n",
      "\n",
      "        [[ 6.5193, -2.2845, 14.5128],\n",
      "         [ 6.5223, -2.2847, 14.5158],\n",
      "         [ 6.5226, -2.2847, 14.5164],\n",
      "         ...,\n",
      "         [ 6.5034, -2.2281, 14.4228],\n",
      "         [ 6.5042, -2.2300, 14.4240],\n",
      "         [ 6.5056, -2.2317, 14.4252]],\n",
      "\n",
      "        [[ 9.1018, -2.1766, 14.6592],\n",
      "         [ 9.1052, -2.1741, 14.6577],\n",
      "         [ 9.1057, -2.1751, 14.6585],\n",
      "         ...,\n",
      "         [ 8.9960, -2.1929, 14.6270],\n",
      "         [ 8.9984, -2.1932, 14.6267],\n",
      "         [ 9.0009, -2.1934, 14.6265]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.9907, -1.2219,  9.4424],\n",
      "         [ 4.9945, -1.2200,  9.4414],\n",
      "         [ 4.9949, -1.2209,  9.4407],\n",
      "         ...,\n",
      "         [ 4.8835, -1.2144,  9.4023],\n",
      "         [ 4.8857, -1.2153,  9.4023],\n",
      "         [ 4.8881, -1.2161,  9.4021]],\n",
      "\n",
      "        [[ 1.0864, -1.7246, 15.1478],\n",
      "         [ 1.0872, -1.7256, 15.1520],\n",
      "         [ 1.0873, -1.7256, 15.1527],\n",
      "         ...,\n",
      "         [ 1.1480, -1.6924, 15.0541],\n",
      "         [ 1.1470, -1.6938, 15.0558],\n",
      "         [ 1.1463, -1.6956, 15.0575]],\n",
      "\n",
      "        [[ 6.3380, -1.4185, 11.4569],\n",
      "         [ 6.3372, -1.4185, 11.4611],\n",
      "         [ 6.3369, -1.4185, 11.4608],\n",
      "         ...,\n",
      "         [ 6.4347, -1.4273, 11.3929],\n",
      "         [ 6.4329, -1.4283, 11.3947],\n",
      "         [ 6.4314, -1.4288, 11.3966]]], device='cuda:0')\n",
      "tensor(3.4564, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "\n",
    "small_output = []\n",
    "small_y = []\n",
    "for i in range(100):\n",
    "    input = torch.from_numpy(train_x[i]).to(device)\n",
    "    pred = forward_model(model, input, K,\n",
    "                        det_thresh=0.3,\n",
    "                        nms_kernel_size=1)\n",
    "    small_output.append(pred)\n",
    "    small_y.append(train_y[i])\n",
    "\n",
    "aligned_x, aligned_y = align_humans(small_output, small_y)\n",
    "print(compute_loss(aligned_x, aligned_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss on entire dataset with batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\np_x, p_y = None, None\\nK = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\\nlosses = []\\nbatch_size = 100\\nnum_batches = len(train_x) // batch_size\\nremainder = len(train_x) % batch_size\\n\\ndef process_batch(batch):\\n    batch_output = []\\n    for image in batch:\\n        input = torch.from_numpy(image).to(device)\\n        pred = forward_model(model, input, K,\\n                            det_thresh=0.3,\\n                            nms_kernel_size=1)\\n        batch_output.append(pred)\\n    return batch_output\\n\\nfor i in range(num_batches):\\n    batch_x = train_x[i * batch_size: (i + 1) * batch_size]\\n    batch_y = train_y[i * batch_size: (i + 1) * batch_size]\\n    batch_output = process_batch(batch_x)\\n    batch_output, batch_y = align_humans(batch_output, batch_y)\\n    batch_loss = compute_loss(batch_output, batch_y)\\n    losses.append(batch_loss)\\n    torch.cuda.empty_cache()\\n    print(f\"{(i+1)*100} of {len(train_x)}\")\\n\\nif remainder > 0:\\n    remainder_x = train_x[num_batches * batch_size: ]\\n    remainder_y = train_y[num_batches * batch_size: ]\\n    remainder_output = process_batch(remainder_x)\\n    remainder_output, remainder_y = align_humans(remainder_output, remainder_y)\\n    remainder_loss = compute_loss(remainder_output, remainder_y)\\n    losses.append(remainder_loss)\\n\\ntotal_loss = sum(losses)\\naverage_loss = total_loss / len(train_x)\\nprint(average_loss)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "losses = []\n",
    "batch_size = 100\n",
    "num_batches = len(train_x) // batch_size\n",
    "remainder = len(train_x) % batch_size\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch_output = []\n",
    "    for image in batch:\n",
    "        input = torch.from_numpy(image).to(device)\n",
    "        pred = forward_model(model, input, K,\n",
    "                            det_thresh=0.3,\n",
    "                            nms_kernel_size=1)\n",
    "        batch_output.append(pred)\n",
    "    return batch_output\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_x = train_x[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_y = train_y[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_output = process_batch(batch_x)\n",
    "    batch_output, batch_y = align_humans(batch_output, batch_y)\n",
    "    batch_loss = compute_loss(batch_output, batch_y)\n",
    "    losses.append(batch_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"{(i+1)*100} of {len(train_x)}\")\n",
    "\n",
    "if remainder > 0:\n",
    "    remainder_x = train_x[num_batches * batch_size: ]\n",
    "    remainder_y = train_y[num_batches * batch_size: ]\n",
    "    remainder_output = process_batch(remainder_x)\n",
    "    remainder_output, remainder_y = align_humans(remainder_output, remainder_y)\n",
    "    remainder_loss = compute_loss(remainder_output, remainder_y)\n",
    "    losses.append(remainder_loss)\n",
    "\n",
    "total_loss = sum(losses)\n",
    "average_loss = total_loss / len(train_x)\n",
    "print(average_loss)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMPL Params loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smpl_params_loss(predictions, gts, fields, gt_fields):\n",
    "    # assume top dimension of gts is grouped by fields (ex: [[list of poses],[list of betas], etc.])\n",
    "    all_preds = []\n",
    "\n",
    "    for field in fields:\n",
    "        preds = []\n",
    "        for humans in predictions:\n",
    "            pred_values = []\n",
    "            for human in humans:\n",
    "                pred_values.append(human[field])\n",
    "            preds.append(pred_values)\n",
    "        all_preds.append(preds)\n",
    "    \n",
    "    criterion = torch.nn.L1Loss()\n",
    "    losses = []\n",
    "\n",
    "    for i, field in enumerate(fields):\n",
    "        # convert to tensors\n",
    "        preds[i] = torch.stack([tensor for sublist in preds[i] for tensor in sublist])\n",
    "        gts[i] = torch.from_numpy(np.stack([item for sublist in gts[i] for item in sublist]))\n",
    "\n",
    "        preds[i] = preds.to(device)\n",
    "        gts[i] = gts.to(device)\n",
    "\n",
    "        losses.append(criterion(preds[i], gts[i]))\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'x_attention_head' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize using loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 2.177070GB\n",
      "torch.cuda.memory_reserved: 2.546875GB\n",
      "torch.cuda.max_memory_reserved: 3.455078GB\n",
      "number of humans detected: 146\n",
      "Epoch 1, Batch 1, Loss: 0.1316379400867566\n",
      "number of humans detected: 122\n",
      "Epoch 1, Batch 2, Loss: 1.0699504508243423\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#model = load_model('multiHMR_896_L')\n",
    "\n",
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "batch_size = 20\n",
    "num_epochs = 10  \n",
    "num_batches = len(train_x) // batch_size\n",
    "remainder = len(train_x) % batch_size\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "# Shuffle Data\n",
    "combined = list(zip(train_x, train_y))\n",
    "random.shuffle(combined)\n",
    "train_x[:], train_y[:] = zip(*combined)\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch_output = []\n",
    "    for image in batch:\n",
    "        input = torch.from_numpy(image).to(device)\n",
    "        pred = model(input, \n",
    "                           is_training=True, \n",
    "                           nms_kernel_size=1,\n",
    "                           det_thresh=0.4,\n",
    "                           K=K)\n",
    "        batch_output.append(pred)\n",
    "    return batch_output\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for i in range(num_batches):\n",
    "        batch_x = train_x[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_y = train_y[i * batch_size: (i + 1) * batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_output = process_batch(batch_x)\n",
    "        batch_output, batch_y = align_humans(batch_output, batch_y)\n",
    "        batch_loss = compute_loss(batch_output, batch_y)\n",
    "\n",
    "        if torch.isnan(batch_loss):\n",
    "            print(f\"NaN detected in loss at batch {i}\")\n",
    "            break\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(batch_loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Batch {(i + 1)}, Loss: {batch_loss.item()/batch_size}\")\n",
    "        del batch_x, batch_y, batch_output, batch_loss\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if remainder > 0:\n",
    "        remainder_x = train_x[num_batches * batch_size:]\n",
    "        remainder_y = train_y[num_batches * batch_size:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        remainder_output = process_batch(remainder_x)\n",
    "        remainder_output, remainder_y = align_humans(remainder_output, remainder_y)\n",
    "        remainder_loss = compute_loss(remainder_output, remainder_y)\n",
    "\n",
    "        remainder_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(remainder_loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Remainder Batch, Loss: {remainder_loss.item()/batch_size}\")\n",
    "\n",
    "        del remainder_x, remainder_y, remainder_output, remainder_loss\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "    total_epoch_loss = sum(epoch_losses)\n",
    "    average_epoch_loss = total_epoch_loss / len(train_x)\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {average_epoch_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas\n",
    "with open(\"Panda.pkl\", 'rb') as f:\n",
    "    data = pandas.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/Det/01_University_Canteen/IMG_01_01.jpg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['img_path']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multihmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
