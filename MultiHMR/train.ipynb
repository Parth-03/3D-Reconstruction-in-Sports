{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from utils import normalize_rgb, render_meshes, get_focalLength_from_fieldOfView, demo_color as color, print_distance_on_image, render_side_views, create_scene, MEAN_PARAMS, CACHE_DIR_MULTIHMR, SMPLX_DIR\n",
    "from model import Model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions from demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(img_path, img_size, device=torch.device('cuda')):\n",
    "    \"\"\" Open image at path, resize and pad \"\"\"\n",
    "\n",
    "    # Open and reshape\n",
    "    img_pil = Image.open(img_path).convert('RGB')\n",
    "    img_pil = ImageOps.contain(img_pil, (img_size,img_size)) # keep the same aspect ratio\n",
    "\n",
    "    # Keep a copy for visualisations.\n",
    "    img_pil_bis = ImageOps.pad(img_pil.copy(), size=(img_size,img_size), color=(255, 255, 255))\n",
    "    img_pil = ImageOps.pad(img_pil, size=(img_size,img_size)) # pad with zero on the smallest side\n",
    "\n",
    "    # Go to numpy \n",
    "    resize_img = np.asarray(img_pil)\n",
    "\n",
    "    # Normalize and go to torch. MODIFIED TO NOT GOT TO TORCH\n",
    "    resize_img = normalize_rgb(resize_img)\n",
    "    x = np.expand_dims(resize_img, axis=0)\n",
    "    return x, img_pil_bis\n",
    "\n",
    "def load_model(model_name, device=torch.device('cuda')):\n",
    "    \"\"\" Open a checkpoint, build Multi-HMR using saved arguments, load the model weigths. \"\"\"\n",
    "    # Model\n",
    "    ckpt_path = os.path.join(CACHE_DIR_MULTIHMR, model_name+ '.pt')\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        os.makedirs(CACHE_DIR_MULTIHMR, exist_ok=True)\n",
    "        print(f\"{ckpt_path} not found...\")\n",
    "        print(\"It should be the first time you run the demo code\")\n",
    "        print(\"Downloading checkpoint from NAVER LABS Europe website...\")\n",
    "        \n",
    "        try:\n",
    "            os.system(f\"wget -O {ckpt_path} https://download.europe.naverlabs.com/ComputerVision/MultiHMR/{model_name}.pt\")\n",
    "            print(f\"Ckpt downloaded to {ckpt_path}\")\n",
    "        except:\n",
    "            assert \"Please contact fabien.baradel@naverlabs.com or open an issue on the github repo\"\n",
    "\n",
    "    # Load weights\n",
    "    print(\"Loading model\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # Get arguments saved in the checkpoint to rebuild the model\n",
    "    kwargs = {}\n",
    "    for k,v in vars(ckpt['args']).items():\n",
    "            kwargs[k] = v\n",
    "\n",
    "    # Build the model.\n",
    "    kwargs['type'] = ckpt['args'].train_return_type\n",
    "    kwargs['img_size'] = ckpt['args'].img_size[0]\n",
    "    model = Model(**kwargs).to(device)\n",
    "\n",
    "    # Load weights into model.\n",
    "    model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    print(\"Weights have been loaded\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def forward_model(model, input_image, camera_parameters,\n",
    "                  det_thresh=0.3,\n",
    "                  nms_kernel_size=1,\n",
    "                 ):\n",
    "        \n",
    "    \"\"\" Make a forward pass on an input image and camera parameters. \"\"\"\n",
    "    \n",
    "    # Forward the model.\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            humans = model(input_image, \n",
    "                           is_training=False, \n",
    "                           nms_kernel_size=int(nms_kernel_size),\n",
    "                           det_thresh=det_thresh,\n",
    "                           K=camera_parameters)\n",
    "\n",
    "    return humans\n",
    "\n",
    "def get_camera_parameters(img_size, fov=60, p_x=None, p_y=None, device=torch.device('cuda')):\n",
    "    \"\"\" Given image size, fov and principal point coordinates, return K the camera parameter matrix\"\"\"\n",
    "    K = torch.eye(3)\n",
    "    # Get focal length.\n",
    "    focal = get_focalLength_from_fieldOfView(fov=fov, img_size=img_size)\n",
    "    K[0,0], K[1,1] = focal, focal\n",
    "\n",
    "    # Set principal point\n",
    "    if p_x is not None and p_y is not None:\n",
    "            K[0,-1], K[1,-1] = p_x * img_size, p_y * img_size\n",
    "    else:\n",
    "            K[0,-1], K[1,-1] = img_size//2, img_size//2\n",
    "\n",
    "    # Add batch dimension\n",
    "    K = K.unsqueeze(0).to(device)\n",
    "    return K\n",
    "\n",
    "def overlay_human_meshes(humans, K, model, img_pil, unique_color=False):\n",
    "\n",
    "    # Color of humans seen in the image.\n",
    "    _color = [color[0] for _ in range(len(humans))] if unique_color else color\n",
    "    \n",
    "    # Get focal and princpt for rendering.\n",
    "    focal = np.asarray([K[0,0,0].cpu().numpy(),K[0,1,1].cpu().numpy()])\n",
    "    princpt = np.asarray([K[0,0,-1].cpu().numpy(),K[0,1,-1].cpu().numpy()])\n",
    "\n",
    "    # Get the vertices produced by the model.\n",
    "    verts_list = [humans[j]['verts_smplx'].cpu().numpy() for j in range(len(humans))]\n",
    "    faces_list = [model.smpl_layer['neutral'].bm_x.faces for j in range(len(humans))]\n",
    "\n",
    "    # Render the meshes onto the image.\n",
    "    pred_rend_array = render_meshes(np.asarray(img_pil), \n",
    "            verts_list,\n",
    "            faces_list,\n",
    "            {'focal': focal, 'princpt': princpt},\n",
    "            alpha=1.0,\n",
    "            color=_color)\n",
    "\n",
    "    return pred_rend_array, _color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/scott/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have been loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_model('multiHMR_896_L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AGORA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = model.img_size\n",
    "\n",
    "train_x_path = \"AGORA/train_0\"\n",
    "train_x = [] # images\n",
    "train_y = [] # ground truth mesh vertices\n",
    "\n",
    "with open(\"AGORA/SMPLX/train_0_withjv.pkl\", \"rb\") as file:\n",
    "    df = pd.read_pickle(file)\n",
    "    for filename in os.listdir(train_x_path):\n",
    "        file_path = os.path.join(train_x_path, filename)\n",
    "        x, img_pil_nopad = open_image(file_path, img_size)\n",
    "        train_x.append(x)\n",
    "        y = df[df['imgPath'] == filename.replace(\"_1280x720\", \"\")]\n",
    "        train_y.append(np.array(y['gt_verts'][0]))\n",
    "\n",
    "assert len(train_x) == len(train_y) == 1453 # Size of AGORA/train0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertices Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra humans based on detection score\n",
    "from copy import copy\n",
    "def align_humans(predictions, gts):\n",
    "    predictions = copy(predictions)\n",
    "    gts = copy(gts)\n",
    "\n",
    "    aligned_preds = []\n",
    "    aligned_gts = []\n",
    "    for pred, gt in zip(predictions, gts):\n",
    "        while len(pred) > len(gt):\n",
    "            det_scores = [person['scores'] for person in pred]\n",
    "            min_value = min(det_scores, key=lambda x: x.item())\n",
    "            index = det_scores.index(min_value)\n",
    "            pred = pred[:index] + pred[index+1:]\n",
    "        while len(pred) < len(gt):\n",
    "            gt = gt[:len(gt)-1]\n",
    "            \n",
    "        assert len(pred) == len(gt)\n",
    "        aligned_preds.append(pred)\n",
    "        aligned_gts.append(gt)\n",
    "\n",
    "    assert len(aligned_preds) == len(aligned_gts)\n",
    "    return aligned_preds, aligned_gts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, gts):\n",
    "    # generate array of only vertex information\n",
    "    pred_humans_with_only_vertices = []\n",
    "\n",
    "    count = 0\n",
    "    for humans in predictions:\n",
    "        pred_vertices = []\n",
    "        for human in humans:\n",
    "            pred_vertices.append(human['verts_smplx'])\n",
    "            count += 1\n",
    "        pred_humans_with_only_vertices.append(pred_vertices)\n",
    "    \n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    # convert to tensors\n",
    "    pred_humans_with_only_vertices = torch.stack([tensor for sublist in pred_humans_with_only_vertices for tensor in sublist])\n",
    "    gts = torch.from_numpy(np.stack([item for sublist in gts for item in sublist]))\n",
    "\n",
    "    pred_humans_with_only_vertices = pred_humans_with_only_vertices.to(device)\n",
    "    gts = gts.to(device)\n",
    "\n",
    "    return criterion(pred_humans_with_only_vertices, gts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711\n",
      "tensor(3.4564, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "\n",
    "small_output = []\n",
    "small_y = []\n",
    "for i in range(100):\n",
    "    input = torch.from_numpy(train_x[i]).to(device)\n",
    "    pred = forward_model(model, input, K,\n",
    "                        det_thresh=0.3,\n",
    "                        nms_kernel_size=1)\n",
    "    small_output.append(pred)\n",
    "    small_y.append(train_y[i])\n",
    "\n",
    "aligned_x, aligned_y = align_humans(small_output, small_y)\n",
    "print(compute_loss(aligned_x, aligned_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss on entire dataset with batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711\n",
      "100 of 1453\n",
      "751\n",
      "200 of 1453\n",
      "750\n",
      "300 of 1453\n",
      "739\n",
      "400 of 1453\n",
      "770\n",
      "500 of 1453\n",
      "735\n",
      "600 of 1453\n",
      "733\n",
      "700 of 1453\n",
      "752\n",
      "800 of 1453\n",
      "741\n",
      "900 of 1453\n",
      "770\n",
      "1000 of 1453\n",
      "736\n",
      "1100 of 1453\n",
      "732\n",
      "1200 of 1453\n",
      "765\n",
      "1300 of 1453\n",
      "753\n",
      "1400 of 1453\n",
      "383\n",
      "tensor(51.1631, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "losses = []\n",
    "batch_size = 100\n",
    "num_batches = len(train_x) // batch_size\n",
    "remainder = len(train_x) % batch_size\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch_output = []\n",
    "    for image in batch:\n",
    "        input = torch.from_numpy(image).to(device)\n",
    "        pred = forward_model(model, input, K,\n",
    "                            det_thresh=0.3,\n",
    "                            nms_kernel_size=1)\n",
    "        batch_output.append(pred)\n",
    "    return batch_output\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_x = train_x[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_y = train_y[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_output = process_batch(batch_x)\n",
    "    batch_output, batch_y = align_humans(batch_output, batch_y)\n",
    "    batch_loss = compute_loss(batch_output, batch_y)\n",
    "    losses.append(batch_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"{(i+1)*100} of {len(train_x)}\")\n",
    "\n",
    "if remainder > 0:\n",
    "    remainder_x = train_x[num_batches * batch_size: ]\n",
    "    remainder_y = train_y[num_batches * batch_size: ]\n",
    "    remainder_output = process_batch(remainder_x)\n",
    "    remainder_output, remainder_y = align_humans(remainder_output, remainder_y)\n",
    "    remainder_loss = compute_loss(remainder_output, remainder_y)\n",
    "    losses.append(remainder_loss)\n",
    "\n",
    "total_loss = sum(losses)\n",
    "average_loss = total_loss / len(train_x)\n",
    "print(average_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 6.537454GB\n",
      "torch.cuda.memory_reserved: 6.960938GB\n",
      "torch.cuda.max_memory_reserved: 6.962891GB\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 6.54 GiB already allocated; 61.38 MiB free; 6.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m train_y[i \u001b[38;5;241m*\u001b[39m batch_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m batch_output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m batch_output, batch_y \u001b[38;5;241m=\u001b[39m align_humans(batch_output, batch_y)\n\u001b[1;32m     41\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m compute_loss(batch_output, batch_y)\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m batch_output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m, \n\u001b[1;32m     24\u001b[0m                        is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     25\u001b[0m                        nms_kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     26\u001b[0m                        det_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m,\n\u001b[1;32m     27\u001b[0m                        K\u001b[38;5;241m=\u001b[39mK)\n\u001b[1;32m     28\u001b[0m     batch_output\u001b[38;5;241m.\u001b[39mappend(pred)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 6.54 GiB already allocated; 61.38 MiB free; 6.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "batch_size = 10\n",
    "num_epochs = 10  \n",
    "num_batches = len(train_x) // batch_size\n",
    "remainder = len(train_x) % batch_size\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch_output = []\n",
    "    for image in batch:\n",
    "        input = torch.from_numpy(image).to(device)\n",
    "        pred = model(input, \n",
    "                           is_training=True, \n",
    "                           nms_kernel_size=1,\n",
    "                           det_thresh=0.4,\n",
    "                           K=K)\n",
    "        batch_output.append(pred)\n",
    "    return batch_output\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for i in range(num_batches):\n",
    "        batch_x = train_x[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_y = train_y[i * batch_size: (i + 1) * batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_output = process_batch(batch_x)\n",
    "        batch_output, batch_y = align_humans(batch_output, batch_y)\n",
    "        batch_loss = compute_loss(batch_output, batch_y)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(batch_loss.item())\n",
    "        \n",
    "        del batch_x, batch_y, batch_output, batch_loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Batch {(i + 1)}, Loss: {batch_loss.item()}\")\n",
    "\n",
    "    if remainder > 0:\n",
    "        remainder_x = train_x[num_batches * batch_size:]\n",
    "        remainder_y = train_y[num_batches * batch_size:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        remainder_output = process_batch(remainder_x)\n",
    "        remainder_output, remainder_y = align_humans(remainder_output, remainder_y)\n",
    "        remainder_loss = compute_loss(remainder_output, remainder_y)\n",
    "\n",
    "        remainder_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(remainder_loss.item())\n",
    "\n",
    "        del remainder_x, remainder_y, remainder_output, remainder_loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Remainder Batch, Loss: {remainder_loss.item()}\")\n",
    "\n",
    "    total_epoch_loss = sum(epoch_losses)\n",
    "    average_epoch_loss = total_epoch_loss / len(train_x)\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {average_epoch_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multihmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
