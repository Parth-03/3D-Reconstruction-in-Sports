{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from utils import normalize_rgb, render_meshes, get_focalLength_from_fieldOfView, demo_color as color, print_distance_on_image, render_side_views, create_scene, MEAN_PARAMS, CACHE_DIR_MULTIHMR, SMPLX_DIR\n",
    "from model import Model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions from demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(img_path, img_size, device=torch.device('cuda')):\n",
    "    \"\"\" Open image at path, resize and pad \"\"\"\n",
    "\n",
    "    # Open and reshape\n",
    "    img_pil = Image.open(img_path).convert('RGB')\n",
    "    img_pil = ImageOps.contain(img_pil, (img_size,img_size)) # keep the same aspect ratio\n",
    "\n",
    "    # Keep a copy for visualisations.\n",
    "    img_pil_bis = ImageOps.pad(img_pil.copy(), size=(img_size,img_size), color=(255, 255, 255))\n",
    "    img_pil = ImageOps.pad(img_pil, size=(img_size,img_size)) # pad with zero on the smallest side\n",
    "\n",
    "    # Go to numpy \n",
    "    resize_img = np.asarray(img_pil)\n",
    "\n",
    "    # Normalize and go to torch. MODIFIED TO NOT GOT TO TORCH\n",
    "    resize_img = normalize_rgb(resize_img)\n",
    "    x = np.expand_dims(resize_img, axis=0)\n",
    "    return x, img_pil_bis\n",
    "\n",
    "def load_model(model_name, device=torch.device('cuda')):\n",
    "    \"\"\" Open a checkpoint, build Multi-HMR using saved arguments, load the model weigths. \"\"\"\n",
    "    # Model\n",
    "    ckpt_path = os.path.join(CACHE_DIR_MULTIHMR, model_name+ '.pt')\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        os.makedirs(CACHE_DIR_MULTIHMR, exist_ok=True)\n",
    "        print(f\"{ckpt_path} not found...\")\n",
    "        print(\"It should be the first time you run the demo code\")\n",
    "        print(\"Downloading checkpoint from NAVER LABS Europe website...\")\n",
    "        \n",
    "        try:\n",
    "            os.system(f\"wget -O {ckpt_path} https://download.europe.naverlabs.com/ComputerVision/MultiHMR/{model_name}.pt\")\n",
    "            print(f\"Ckpt downloaded to {ckpt_path}\")\n",
    "        except:\n",
    "            assert \"Please contact fabien.baradel@naverlabs.com or open an issue on the github repo\"\n",
    "\n",
    "    # Load weights\n",
    "    print(\"Loading model\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # Get arguments saved in the checkpoint to rebuild the model\n",
    "    kwargs = {}\n",
    "    for k,v in vars(ckpt['args']).items():\n",
    "            kwargs[k] = v\n",
    "\n",
    "    # Build the model.\n",
    "    kwargs['type'] = ckpt['args'].train_return_type\n",
    "    kwargs['img_size'] = ckpt['args'].img_size[0]\n",
    "    model = Model(**kwargs).to(device)\n",
    "\n",
    "    # Load weights into model.\n",
    "    model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "    print(\"Weights have been loaded\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def forward_model(model, input_image, camera_parameters,\n",
    "                  det_thresh=0.3,\n",
    "                  nms_kernel_size=1,\n",
    "                 ):\n",
    "        \n",
    "    \"\"\" Make a forward pass on an input image and camera parameters. \"\"\"\n",
    "    \n",
    "    # Forward the model.\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            humans = model(input_image, \n",
    "                           is_training=False, \n",
    "                           nms_kernel_size=int(nms_kernel_size),\n",
    "                           det_thresh=det_thresh,\n",
    "                           K=camera_parameters)\n",
    "\n",
    "    return humans\n",
    "\n",
    "def get_camera_parameters(img_size, fov=60, p_x=None, p_y=None, device=torch.device('cuda')):\n",
    "    \"\"\" Given image size, fov and principal point coordinates, return K the camera parameter matrix\"\"\"\n",
    "    K = torch.eye(3)\n",
    "    # Get focal length.\n",
    "    focal = get_focalLength_from_fieldOfView(fov=fov, img_size=img_size)\n",
    "    K[0,0], K[1,1] = focal, focal\n",
    "\n",
    "    # Set principal point\n",
    "    if p_x is not None and p_y is not None:\n",
    "            K[0,-1], K[1,-1] = p_x * img_size, p_y * img_size\n",
    "    else:\n",
    "            K[0,-1], K[1,-1] = img_size//2, img_size//2\n",
    "\n",
    "    # Add batch dimension\n",
    "    K = K.unsqueeze(0).to(device)\n",
    "    return K\n",
    "\n",
    "def overlay_human_meshes(humans, K, model, img_pil, unique_color=False):\n",
    "\n",
    "    # Color of humans seen in the image.\n",
    "    _color = [color[0] for _ in range(len(humans))] if unique_color else color\n",
    "    \n",
    "    # Get focal and princpt for rendering.\n",
    "    focal = np.asarray([K[0,0,0].cpu().numpy(),K[0,1,1].cpu().numpy()])\n",
    "    princpt = np.asarray([K[0,0,-1].cpu().numpy(),K[0,1,-1].cpu().numpy()])\n",
    "\n",
    "    # Get the vertices produced by the model.\n",
    "    verts_list = [humans[j]['verts_smplx'].cpu().numpy() for j in range(len(humans))]\n",
    "    faces_list = [model.smpl_layer['neutral'].bm_x.faces for j in range(len(humans))]\n",
    "\n",
    "    # Render the meshes onto the image.\n",
    "    pred_rend_array = render_meshes(np.asarray(img_pil), \n",
    "            verts_list,\n",
    "            faces_list,\n",
    "            {'focal': focal, 'princpt': princpt},\n",
    "            alpha=1.0,\n",
    "            color=_color)\n",
    "\n",
    "    return pred_rend_array, _color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/scott/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/scott/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have been loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_model('multiHMR_896_L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AGORA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = model.img_size\n",
    "\n",
    "train_x_path = \"AGORA/train_0\"\n",
    "train_x = [] # images\n",
    "train_y = [] # ground truth mesh vertices\n",
    "\n",
    "with open(\"AGORA/SMPLX/train_0_withjv.pkl\", \"rb\") as file:\n",
    "    df = pd.read_pickle(file)\n",
    "    for filename in os.listdir(train_x_path):\n",
    "        file_path = os.path.join(train_x_path, filename)\n",
    "        x, img_pil_nopad = open_image(file_path, img_size)\n",
    "        train_x.append(x)\n",
    "        y = df[df['imgPath'] == filename.replace(\"_1280x720\", \"\")]\n",
    "        train_y.append(np.array(y['gt_verts'][0]))\n",
    "\n",
    "assert len(train_x) == len(train_y) == 1453 # Size of AGORA/train0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra humans based on detection score\n",
    "def align_humans(predictions, gts):\n",
    "    for pred, gt in zip(predictions, gts):\n",
    "        while len(pred) > len(gt):\n",
    "            det_scores = [person['scores'] for person in pred]\n",
    "            min_value = min(det_scores, key=lambda x: x.item())\n",
    "            index = det_scores.index(min_value)\n",
    "            del pred[index]\n",
    "        assert len(pred) <= len(gt)\n",
    "\n",
    "    return predictions, gts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, gts):\n",
    "    # generate array of only vertex information\n",
    "    pred_humans_with_only_vertices = []\n",
    "\n",
    "    for humans in predictions:\n",
    "        pred_vertices = []\n",
    "        for human in humans:\n",
    "            pred_vertices.append(human['verts_smplx'])\n",
    "        pred_humans_with_only_vertices.append(pred_vertices)\n",
    "    \n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    # convert to tensors\n",
    "    pred_humans_with_only_vertices = torch.stack([tensor for sublist in pred_humans_with_only_vertices for tensor in sublist])\n",
    "    gts = torch.from_numpy(np.stack([item for sublist in gts for item in sublist]))\n",
    "\n",
    "    pred_humans_with_only_vertices = pred_humans_with_only_vertices.to(device)\n",
    "    gts = gts.to(device)\n",
    "\n",
    "    return criterion(pred_humans_with_only_vertices, gts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0467, device='cuda:0', dtype=torch.float64)\n",
      "tensor(3.0467, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "\n",
    "small_output = []\n",
    "small_y = []\n",
    "for i in range(5):\n",
    "    input = torch.from_numpy(train_x[i]).to(device)\n",
    "    pred = forward_model(model, input, K,\n",
    "                        det_thresh=0.3,\n",
    "                        nms_kernel_size=1)\n",
    "    small_output.append(pred)\n",
    "    small_y.append(train_y[i])\n",
    "\n",
    "aligned_x, aligned_y = align_humans(small_output, small_y)\n",
    "print(compute_loss(aligned_x, aligned_y))\n",
    "print(compute_loss(small_output, small_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loss on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 282.00 MiB (GPU 0; 7.79 GiB total capacity; 3.31 GiB already allocated; 311.69 MiB free; 3.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m train_x:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdet_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnms_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(compute_loss(output, train_y))\n",
      "Cell \u001b[0;32mIn[27], line 66\u001b[0m, in \u001b[0;36mforward_model\u001b[0;34m(model, input_image, camera_parameters, det_thresh, nms_kernel_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 66\u001b[0m         humans \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnms_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnms_kernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdet_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdet_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcamera_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m humans\n",
      "File \u001b[0;32m~/anaconda3/envs/multihmr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Final Project/multi-hmr/model.py:195\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x, idx, det_thresh, nms_kernel_size, K, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m loc \u001b[38;5;241m=\u001b[39m (loc \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m offset ) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# SMPL parameter regression\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[43mz_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# retrieving dense features associated to each central vector\u001b[39;00m\n\u001b[1;32m    196\u001b[0m pred_smpl_params, pred_cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_attention_head(z_central, kv, idx_0\u001b[38;5;241m=\u001b[39midx[\u001b[38;5;241m0\u001b[39m], idx_det\u001b[38;5;241m=\u001b[39midx)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Get outputs from the SMPL layer.\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 282.00 MiB (GPU 0; 7.79 GiB total capacity; 3.31 GiB already allocated; 311.69 MiB free; 3.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "p_x, p_y = None, None\n",
    "K = get_camera_parameters(model.img_size, fov=60, p_x=p_x, p_y=p_y)\n",
    "\n",
    "output = []\n",
    "for image in train_x:\n",
    "    input = torch.from_numpy(image).to(device)\n",
    "    pred = forward_model(model, input, K,\n",
    "                         det_thresh=0.3,\n",
    "                         nms_kernel_size=1)\n",
    "    output.append(pred)\n",
    "\n",
    "print(compute_loss(output, train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multihmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
