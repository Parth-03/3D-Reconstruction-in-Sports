{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project setup\n",
        "\n",
        "[Project repo link](https://github.com/nicolasugrinovic/size_depth_disambiguation?tab=readme-ov-file)\n",
        "\n",
        "\n",
        "[Paper link](https://www.iri.upc.edu/people/nugrinovic/depthsize/paper.pdf)\n",
        "\n",
        "You should run this in GPU runtime.\n"
      ],
      "metadata": {
        "id": "HckNb7E-JqCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install python packages"
      ],
      "metadata": {
        "id": "JMXCCNr5Kx2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "f4nVqArEx51H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIJCHmWMIskf"
      },
      "outputs": [],
      "source": [
        "# Dependencies from requirements.txt & packages needed to install pytorch3d\n",
        "!pip install trimesh open3d pyransac3d fvcore iopath timm\n",
        "\n",
        "# pytorch3d\n",
        "import sys\n",
        "import torch\n",
        "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{pyt_version_str}\"\n",
        "])\n",
        "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "\n",
        "# Detectron2: source - https://detectron2.readthedocs.io/en/latest/tutorials/install.html\n",
        "# THIS STEP TAKES LONG (7-8 min)\n",
        "!python -m pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "# panoptic api\n",
        "!pip install 'git+https://github.com/cocodataset/panopticapi.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone project repo and download external tools"
      ],
      "metadata": {
        "id": "5-8pW-TLK2iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone project repo\n",
        "!git clone https://github.com/yunijeong5/size_depth_disambiguation.git\n",
        "\n",
        "# make checkpoints directory to put model\n",
        "%mkdir /content/size_depth_disambiguation/external/panoptic_deeplab/tools_d2/checkpoints"
      ],
      "metadata": {
        "id": "mheTpfILdE7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the cell below, you should download the Panoptic model needed to run the precomputation process. Download the model [here](https://drive.google.com/file/d/17bl_n0SUXVktA0x2507dlwmti7s8FIpe/view?usp=sharing) and save it to your Google Drive."
      ],
      "metadata": {
        "id": "8JLHHI2sycmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy model checkpoint from Google Drive. Replace \"674 Project\" to your folder name.\n",
        "!cp -r \"/content/drive/My Drive/674 Project/panoptic_deeplab_H_48_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.pth\" /content/size_depth_disambiguation/external/panoptic_deeplab/tools_d2/checkpoints/panoptic_deeplab_H_48_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.pth\n",
        "\n",
        "# Download Midas weights\n",
        "%cd /content/size_depth_disambiguation/\n",
        "%mkdir -p ./weights\n",
        "%cd weights\n",
        "!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt"
      ],
      "metadata": {
        "id": "yjvAdl2ByXpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "UqqASOQLo5fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From the Size Depth Disambiguation GitHub:\n",
        "> To run the demo, you first need to generate (or precompute) data which is then used by the optimization method. You need to generate initial pose/shape estimations and give them the correct format.\n",
        "\n",
        "\n",
        "One image file (jpg or png), and one obj file and json file (format: `{\"joints_3d\": [], \"translation\": []}`) are needed for each person in the photo.\n",
        "\n",
        "Speicfy the folder with input images and pose/shape estimation with `--input_path` argument.\n",
        "\n",
        "The image and obj and json files for people in that image should share the same base name.\n",
        "\n",
        "The optimization process (`run_optim_demo.py`) uses `data_reproj_joints.pkl` generated by the precompute process and files in the input data folder.\n",
        "\n",
        "Putting all together, the input folder format shold be:\n",
        "```\n",
        "input\n",
        "|-- data_name\n",
        "    `-- img_name1.jpg\n",
        "    `-- img_name1_TRANS_person0.obj\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    `-- img_name1_TRANS_personN.obj\n",
        "    `-- img_name1_3djoints_0.json\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    \n",
        "    `-- img_name1_3djoints_N.json\n",
        "    `-- img_name2.jpg\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "```"
      ],
      "metadata": {
        "id": "SVCyGalToAoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/size_depth_disambiguation/\n",
        "\n",
        "# precompute the data\n",
        "!python precompute_estimation_data.py --input_path=./input/coco_demo --output_path=./precomputed_data/coco_demo/ --mode=smpl_reproj --dataset=demo --model_type=dpt_large --config-file ./external/panoptic_deeplab/tools_d2/configs/COCO-PanopticSegmentation/panoptic_deeplab_H_48_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml --opts MODEL.WEIGHTS ./external/panoptic_deeplab/tools_d2/checkpoints/panoptic_deeplab_H_48_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.pth"
      ],
      "metadata": {
        "id": "WHYE4ChOo6vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issues:\n",
        "- The reference person is chosen manually: \"use normal point equation to translate the plane to the feet sde ref person\" (\"usa ecuacion punto normal para trasladar el plano a los pie sde ref person\", run_optim_demp.py), \"ref_person = 1\" (run_optim_demp.py).\n",
        "- The scale of reference person it set to 1, and his translation remains the same as the input."
      ],
      "metadata": {
        "id": "z5UZ2nUES-7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run the optimization\n",
        "!python run_optim_demo.py --model_type=dpt_large --input_path=./input/coco_demo --output_path=./output/coco_demo --input=input/coco_demo/*.jpg --mode=smpl_reproj --plane_scale=1.0 --n_iters=2000 --w_ordinal_loss=0 --w_reg_size=0.0"
      ],
      "metadata": {
        "id": "oO5n_-v-eZOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}